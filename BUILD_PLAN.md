# 4-Day Build Plan: Human-Like Long-Term Memory for Conversational AI

> **For complete beginners.** Each day builds on the previous. Focus: memory architecture + logic flow.

---

## ðŸŽ¯ What You're Building (High-Level)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CONVERSATIONAL AI WITH MEMORY                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   User says: "I'm allergic to peanuts, love Thai food, going to Tokyo"  â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  SHORT-TERM (current turn + last 5â€“10 messages)               â”‚     â”‚
â”‚   â”‚  â†’ Immediate context for the response                         â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  MEMORY EXTRACTOR (Gemini or local logic)                     â”‚     â”‚
â”‚   â”‚  â†’ Picks important facts: "allergic: peanuts", "food: Thai",  â”‚     â”‚
â”‚   â”‚    "travel: Tokyo"                                            â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  LONG-TERM SEMANTIC MEMORY                                    â”‚     â”‚
â”‚   â”‚  â†’ Stored by category (personal, food, travel, misc)          â”‚     â”‚
â”‚   â”‚  â†’ Each memory has an embedding (vector) for similarity search â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚  RETRIEVAL (when user asks something)                         â”‚     â”‚
â”‚   â”‚  â†’ Category-aware search â†’ find relevant memories â†’ use them  â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“… Day-by-Day Plan

### **DAY 1: Foundation + Short-Term Memory**

**Goal:** Get a minimal chat running with short-term context. No AI yetâ€”just structure.

#### Tasks
1. **Setup project** (30 min)
   - Create folder structure
   - Set up Python virtual environment
   - Create `requirements.txt`

2. **Define data models** (1 hr)
   - `Message`: `role`, `content`, `timestamp`
   - `ShortTermBuffer`: list of last N messages (e.g. 10)

3. **Implement short-term memory** (1â€“2 hr)
   - Add message to buffer
   - Truncate when full (keep most recent)
   - Format buffer for context (e.g. "User: X\nAssistant: Y\n...")

4. **Simple CLI chat loop** (1 hr)
   - Input â†’ add to buffer â†’ print response (dummy: "Echo: ...") â†’ repeat

**Deliverable:** A running chat that remembers the last 10 messages.

**Logic flow:**
```
User input â†’ Append to ShortTermBuffer â†’ Format context string â†’ (placeholder) â†’ Print response
```

---

### **DAY 2: Long-Term Memory + Categories**

**Goal:** Add persistent storage and category-based organization.

#### Tasks
1. **Define memory schema** (1 hr)
   - `Memory`: `id`, `content`, `category`, `created_at`, `embedding` (placeholder)
   - Categories: `personal`, `food`, `travel`, `misc`

2. **Storage layer** (1â€“2 hr)
   - Use SQLite (simple, no server)
   - Tables: `memories` (id, content, category, embedding_blob, created_at)
   - Functions: `add_memory()`, `get_memories_by_category()`, `get_all_memories()`

3. **Memory extractor (local version)** (2 hr)
   - Rules: e.g. if message contains "I like X", "I'm allergic to X", "I'm going to X" â†’ extract fact
   - Use regex or simple keyword matching (no Gemini yet)
   - Assign category based on keywords

4. **Wire into chat** (1 hr)
   - After each user message â†’ run extractor â†’ save to DB
   - Log: "Stored: [food] likes Thai food"

**Deliverable:** Chat that stores facts in a database by category.

**Logic flow:**
```
User input â†’ ShortTermBuffer â†’ Extract facts (local rules) â†’ Save to SQLite by category
```

---

### **DAY 3: Embeddings, VL-JEPA-Inspired Latent Space, Retrieval**

**Goal:** Semantic search and JEPA-inspired representation.

#### Tasks
1. **Sentence embeddings** (1 hr)
   - Install `sentence-transformers`
   - Use `all-MiniLM-L6-v2` (lightweight, 384-dim)
   - Encode each memory when storing
   - Store embedding as blob in SQLite

2. **VL-JEPA-inspired latent space** (2 hr)
   - **Concept:** VL-JEPA predicts embeddings in a latent space instead of tokens.
   - **Our version:** Treat the embedding space as the "latent space."
   - Add a small **predictor**: given user query embedding, predict a "refined" query embedding that better matches how memories were stored.
   - Simple implementation: `refined = query_emb + alpha * (mean_memory_emb - query_emb)` (optional centering)
   - Or: use a tiny MLP that takes query_emb â†’ predicted_emb (trained on (query, relevant_memory) pairs if you have data; otherwise skip and use raw embeddings)

   - **Beginner-friendly option:** Skip predictor; use standard similarity. Document: "Embeddings live in a JEPA-style latent space (sentence-transformers)." You can add the predictor later.

3. **Category-aware retrieval** (2 hr)
   - Given user query â†’ guess category from keywords (or use embedding similarity to category centroids)
   - Search only in that category first (fast)
   - Fallback: search all memories if category search returns little

4. **Similarity search** (1 hr)
   - Use cosine similarity: `dot(q, m) / (norm(q) * norm(m))`
   - Return top-k memories
   - For SQLite: load embeddings, compute in Python (or use `sqlite-vec` if you want to go fancier)

5. **Use retrieved memories in context** (1 hr)
   - Before calling Gemini: build context = short-term + "Relevant memories:\n" + top memories

**Deliverable:** Retrieval that finds relevant memories by category and similarity.

**Logic flow:**
```
User query â†’ Embed query â†’ (Optional: refine with predictor) â†’ Category-aware search â†’
Top-K memories â†’ Build context â†’ Ready for LLM
```

---

### **DAY 4: Gemini Integration, Compression, Fallbacks**

**Goal:** Full AI responses, memory compression, and robustness.

#### Tasks
1. **Gemini API integration** (1â€“2 hr)
   - Get API key from Google AI Studio
   - Use OpenAI-compatible SDK (OpenRouter)
   - Build prompt: system (instructions) + short-term context + retrieved memories + user message
   - Call Gemini â†’ stream or get response

2. **Upgrade memory extractor with Gemini** (1 hr)
   - Prompt: "From this conversation, extract important factual information about the user. Output JSON: [{content, category}]"
   - Parse JSON â†’ save to DB with embeddings
   - **Fallback:** If API fails â†’ use Day 2 local extractor

3. **Memory compression** (2 hr)
   - When memory count > threshold (e.g. 50) â†’ trigger compression
   - Call Gemini: "Summarize these memories into 3â€“5 concise facts: [list]"
   - Replace old memories with summary (one "compressed" memory)
   - **Fallback:** If API fails â†’ keep oldest memories, add note "Compression skipped"

4. **API-failure tolerance** (1 hr)
   - Wrap all Gemini calls in try/except
   - On failure: use local/deterministic response
   - Log: "Gemini unavailable, using fallback"

5. **Polish & test** (1 hr)
   - Test with 50+ turns
   - Verify retrieval, compression, fallbacks

**Deliverable:** Production-ready system with Gemini, compression, and graceful fallbacks.

**Logic flow:**
```
User query â†’ Retrieve memories â†’ Build prompt â†’ Try Gemini â†’ Success? Use response : Use fallback
New memories â†’ Count > 50? â†’ Try compress via Gemini : Keep as is
```

---

## ðŸ“ Suggested Project Structure

```
Project - Memory/
â”œâ”€â”€ BUILD_PLAN.md          # This file
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                   # OPENROUTER_API_KEY (gitignore this!)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py            # Entry point, chat loop
â”‚   â”œâ”€â”€ config.py          # Constants, thresholds
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ short_term.py  # ShortTermBuffer
â”‚   â”‚   â”œâ”€â”€ long_term.py   # SQLite, Memory model
â”‚   â”‚   â”œâ”€â”€ extractor.py   # Extract facts (local + Gemini)
â”‚   â”‚   â”œâ”€â”€ embeddings.py  # Sentence embeddings + JEPA-inspired logic
â”‚   â”‚   â”œâ”€â”€ retrieval.py   # Category-aware search
â”‚   â”‚   â””â”€â”€ compression.py # Summarize old memories
â”‚   â””â”€â”€ llm/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ openrouter.py  # OpenRouter API + fallback
â”œâ”€â”€ data/
â”‚   â””â”€â”€ memories.db        # SQLite DB (gitignore)
â””â”€â”€ tests/
    â””â”€â”€ (optional)
```

---

## ðŸ§© VL-JEPA Integration Options (Pick One for Day 3)

### Option A: Conceptual (Beginner)
- Use sentence-transformers embeddings.
- Treat them as your "latent space."
- Document: "JEPA-inspired: we operate in embedding space, not token space."
- **Effort:** 0 extra code.

### Option B: Simple Predictor (Intermediate)
- Add a small neural net: `query_embedding â†’ predicted_embedding`.
- Train on pairs (user_question, relevant_memory_embedding) from your own logs.
- Use predicted embedding for retrieval.
- **Effort:** ~2â€“3 hr.

### Option C: Use JEPA Library (Advanced)
- `pip install jepa`
- Adapt their predictor for sentence embeddings.
- **Effort:** ~4+ hr, more reading.

**Recommendation:** Start with **Option A**. Add Option B in a future iteration if you have time.

---

## ðŸ“‹ Prerequisites (Do Before Day 1)

1. **Python 3.10+**  
   - Check: `python3 --version`

2. **Git** (optional but useful)  
   - Check: `git --version`

3. **Google AI Studio account** (for Day 4)  
   - https://openrouter.ai/  
   - Create API key

4. **~2GB disk** for models (sentence-transformers downloads once)

---

## âœ… Daily Checklist

| Day | Done | Deliverable |
|-----|------|-------------|
| 1   | â˜    | Chat with short-term memory (last 10 msgs) |
| 2   | â˜    | Facts extracted and stored in SQLite by category |
| 3   | â˜    | Embeddings + category-aware retrieval working |
| 4   | â˜    | Gemini + compression + fallbacks working |

---

## ðŸ†˜ If You Get Stuck

- **Day 1:** Focus on "list of messages" and a simple loop. Ignore everything else.
- **Day 2:** SQLite is just `sqlite3` in Python. One table is enough.
- **Day 3:** Use `model.encode(["text"])` from sentence-transformers. Similarity = `np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))`.
- **Day 4:** Gemini SDK has a `generate_content()` method. Wrap it in try/except.

---

## ðŸ”‘ Key Logic Flows (Reference)

### Storing a memory
```
User message â†’ Extractor (Gemini or local) â†’ List of {content, category}
  â†’ For each: encode to embedding â†’ Save to DB
```

### Retrieving for a query
```
User query â†’ Embed query â†’ Infer category (or use all)
  â†’ Load memories from category (or all)
  â†’ Compute similarities â†’ Sort â†’ Top K â†’ Return
```

### Generating response
```
User query
  â†’ Retrieve top-K memories
  â†’ Format: system + short-term + memories + query
  â†’ Gemini API (or fallback)
  â†’ Stream/return response
```

### Compressing
```
If memory_count > 50:
  â†’ Get oldest 20â€“30 memories
  â†’ Gemini: "Summarize into 3â€“5 facts"
  â†’ Delete those, insert summary as 1 memory
```

---

Good luck! Start with Day 1 and move step by step.
